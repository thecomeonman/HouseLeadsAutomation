x = vcFacilities,
pattern = ' *$|^ *',
replacement = ''
)
vcFacilities = paste(
vcFacilities[vcFacilities != ''],
collapse = ', '
)
dtTemp = rbind(
dtTemp,
data.table(
Category = 'Facilities',
Value = vcFacilities
)
)
setDT(dtTemp)
dtTemp[, ZZURL := cListing]
dtTemp
}
),
fill = T
)
}
# Cleaning the data
# =============================================================================
if ( exists('dtListings') ) {
# Changing column names from their website name to something
# that R can accept
setnames(
dtListings,
make.names(colnames(dtListings))
)
# Some cleaning up of the data
dtListings[, Value := gsub(x = Value, pattern = '^ *| *$', replacement = '')]
dtListings[, Value := gsub(x = Value, pattern = ' +', replacement = ' ')]
dtListings[, Category := gsub(x = Category, pattern = ' |[[:punct:]]', replacement = '')]
# Going from long format to wide format
dtListings = dcast(dtListings, ZZURL ~ Category, value.var = 'Value')
setDT(dtListings)
setnames(
dtListings,
make.names(colnames(dtListings))
)
# Some extra junk which doesn't get cleaned
dtListings[, Rent := pdPrice2]
dtListings[, pdPrice2 := NULL]
dtListings[, Rent := gsub(x = Rent, pattern = '.*;', replacement = '')]
dtListings[, Rent := gsub(x = Rent, pattern = ' |[[:punct:]]|[[:alpha:]]', replacement = '')]
}
# Automatic processing of the data
# Code is same across websites
# =============================================================================
if ( exists('dtListings') ) {
#  Running the automated filter
dtListings[, ZZStatus := '']
dtListings[, ZZComments := '']
dtFilters = data.table(gsFilters)
if ( nrow(dtFilters) > 0 ) {
for ( i in seq(nrow(dtFilters))) {
dtListings[
grep(
x = get(dtFilters[i, Column]),
pattern = dtFilters[i, Value],
invert = dtFilters[i, Filter]
),
c(
'ZZCalledBy',
'ZZStatus',
'ZZComments'
) := list(
'Automated',
paste0(ZZStatus, dtFilters[i, Status], '; '),
paste0(ZZComments, dtFilters[i, Comment], '; ')
)
]
}
}
rm(dtFilters)
}
dtFilters
for ( i in seq(nrow(dtFilters))) {
cat(i)
dtListings[
grep(
x = get(dtFilters[i, Column]),
pattern = dtFilters[i, Value],
invert = dtFilters[i, Filter]
),
c(
'ZZCalledBy',
'ZZStatus',
'ZZComments'
) := list(
'Automated',
paste0(ZZStatus, dtFilters[i, Status], '; '),
paste0(ZZComments, dtFilters[i, Comment], '; ')
)
]
}
seq(nrow(dtFilters)))
seq(nrow(dtFilters))
i
rm(i)
for ( i in seq(nrow(dtFilters))) {
cat(i)
dtListings[
grep(
x = get(dtFilters[i, Column]),
pattern = dtFilters[i, Value],
invert = dtFilters[i, Filter]
),
c(
'ZZCalledBy',
'ZZStatus',
'ZZComments'
) := list(
'Automated',
paste0(ZZStatus, dtFilters[i, Status], '; '),
paste0(ZZComments, dtFilters[i, Comment], '; ')
)
]
}
i
cat(i)
dtListings[
grep(
x = get(dtFilters[i, Column]),
pattern = dtFilters[i, Value],
invert = dtFilters[i, Filter]
),
c(
'ZZCalledBy',
'ZZStatus',
'ZZComments'
) := list(
'Automated',
paste0(ZZStatus, dtFilters[i, Status], '; '),
paste0(ZZComments, dtFilters[i, Comment], '; ')
)
]
grep(
x = get(dtFilters[i, Column]),
pattern = dtFilters[i, Value],
invert = dtFilters[i, Filter]
)
get(dtFilters[i, Column])
(dtFilters[i, Column])
dtFilters[i, Value]
dtFilters[i, Filter]
dtFilters
for ( i in seq(nrow(dtFilters))) {
cat(i)
dtListings[
grep(
x = get(dtFilters[i, Column]),
pattern = dtFilters[i, Value],
invert = dtFilters[i, Invert]
),
c(
'ZZCalledBy',
'ZZStatus',
'ZZComments'
) := list(
'Automated',
paste0(ZZStatus, dtFilters[i, Status], '; '),
paste0(ZZComments, dtFilters[i, Comment], '; ')
)
]
}
dtLis
dtListings
dtListings
# Putting old and new entries together
dtListings = rbind(
data.frame(gsListings),
dtListings,
fill = T
)
setDT(dtListings)
# Changing order of columns such that user entered columns come last
setcolorder(
dtListings,
c(
grep(colnames(dtListings), pattern = '^ZZ', value = T, invert = T),
grep(colnames(dtListings), pattern = '^ZZ', value = T)
)
)
# Error values are easier on the eye this way
dtListings[dtListings == 'NULL'] = ''
dtListings[dtListings == 'NA'] = ''
dtListings[is.na(dtListings)] = ''
# Deleting previous sheet and adding data as a new sheet
# This is needed in case there are any new
# columns that go added in this iteration
AHH %>%
gs_ws_delete(ws = cResultsSheetName)
AHH <- gs_title(cFileName)
AHH %>%
gs_ws_new(
ws_title = cResultsSheetName,
input = dtListings,
trim = TRUE,
verbose = FALSE
)
AHH %>%
gs_ws_new(
ws_title = cResultsSheetName,
input = dtListings,
trim = TRUE,
verbose = FALSE
)
AHH <- gs_title(cFileName)
AHH %>%
gs_ws_new(
ws_title = cResultsSheetName,
input = dtListings,
trim = TRUE,
verbose = FALSE
)
rm(list = ls())
cRootDirectory = '/home/ask/Desktop/HouseLeadsAutomation'
setwd(cRootDirectory)
setwd('ScrapingScripts')
# rm(list = ls())
library(googlesheets)
library(data.table)
library(dplyr)
library(rjson)
cFileName = 'Automated House Leads 2018'
cResultsSheetName = 'CommonFloor'
cSearchURLPattern = 'commonfloor'
# Getting details from the Google sheet
# Code is same across websites
# =============================================================================
# Reading the entire sheet
AHH <- gs_title(cFileName)
# Reading the list of search URLs from this website
gsWebpageSearch <- AHH %>% gs_read(ws = "QueryURLs")
vcWebpageSearch = gsWebpageSearch$URL
vcWebpageSearch = vcWebpageSearch[
grepl(
tolower(vcWebpageSearch),
pattern = cSearchURLPattern
)
]
# Reading the listings already scraped from this website
gsListings <- AHH %>% gs_read(ws = cResultsSheetName)
vcAlreadySeenListings = gsListings$ZZURL
# Getting the filters to be applied to the new readings which get scraped
gsFilters <- AHH %>% gs_read(ws = paste0(cResultsSheetName, "Filters"))
# rm(list = ls())
library(googlesheets)
library(data.table)
library(dplyr)
library(rjson)
cFileName = 'Automated House Leads 2018'
cResultsSheetName = 'CommonFloor'
cSearchURLPattern = 'commonfloor'
# Getting details from the Google sheet
# Code is same across websites
# =============================================================================
# Reading the entire sheet
AHH <- gs_title(cFileName)
# Reading the list of search URLs from this website
gsWebpageSearch <- AHH %>% gs_read(ws = "QueryURLs")
vcWebpageSearch = gsWebpageSearch$URL
vcWebpageSearch = vcWebpageSearch[
grepl(
tolower(vcWebpageSearch),
pattern = cSearchURLPattern
)
]
# Reading the listings already scraped from this website
gsListings <- AHH %>% gs_read(ws = cResultsSheetName)
vcAlreadySeenListings = gsListings$ZZURL
# Getting the filters to be applied to the new readings which get scraped
gsFilters <- AHH %>% gs_read(ws = paste0(cResultsSheetName, "Filters"))
# Getting the list of properties to scrape by scraping each search URL
# =============================================================================
# We'll populate this vector with the final URLs
vcURLsToScrape = c()
# This site lists a society as one entry which the user can
# click on to see the actual property entries. We'll need
# to scrape these society pages separately after we're done
# with the search URLs. This vector will store them.
vcWebpagesAddToSearch = c()
# counter to loop through all the search URLs
i = 1
repeat {
if ( i > length(vcWebpageSearch) ) {
break
}
print(paste('Searching',vcWebpageSearch[i]))
# if the search page returns multiple pages of results then
# need to go through each of them
iPageNo = 1
vcURLsToScrapeFromThisPage = c()
repeat {
print(paste('Page number',iPageNo))
# Substituting the page number in the search page URL
vcWebpage = readLines(
paste0(
gsub(
x = vcWebpageSearch[i],
pattern = 'page=.*?&',
replacement = paste0('page=',iPageNo, '&')
)
)
)
# Removing all the unnecessary data from this webpage
# and get the section which has the search results
cStringToChopOffAfterSuggestion = 'No more matching properties for your search'
vcWebpage = vcWebpage[
grepl(
x = vcWebpage,
pattern = cStringToChopOffAfterSuggestion
)
]
vcWebpage = gsub(
x = vcWebpage,
pattern = paste0(cStringToChopOffAfterSuggestion,'.*'),
replacement = ''
)
vcWebpage = strsplit(
x = vcWebpage,
split = 'card cards-list'
)
vcWebpage = unlist(vcWebpage)
vcWebpage = vcWebpage[
grepl(
x = vcWebpage,
pattern = 'row listing'
)
]
# Getting the apartment complex pages to search through later
vcWebpagesToSearch = grep(
x = vcWebpage,
pattern = 'View All Properties',
value = T
)
vcWebpagesToSearch = unlist(
strsplit(
vcWebpagesToSearch,
split = 'href'
)
)
vcWebpagesToSearch = grep(
vcWebpagesToSearch,
pattern = 'View All Properties',
value = T
)
vcWebpagesToSearch = gsub(
vcWebpagesToSearch,
pattern = '^="|" target.*',
replacement = ''
)
if ( length( vcWebpagesToSearch) > 0 ) {
print(paste('Will also search', vcWebpagesToSearch))
}
vcWebpagesAddToSearch = c(
vcWebpagesAddToSearch,
vcWebpagesToSearch
)
# Getthing the non-apartment listings out from this page
vcWebpageListings = grep(
x = vcWebpage,
pattern = 'View All Properties',
value = T,
invert = T
)
vcWebpageListings = gsub(
x = vcWebpageListings,
pattern = '.*?href',
replacement = ''
)
vcWebpageListings = gsub(
x = vcWebpageListings,
pattern = 'href ?= ?"(/listing/.*?)".*',
replacement = '\\1'
)
vcWebpageListings = vcWebpageListings[nchar(vcWebpageListings)>0]
if ( length(vcWebpageListings) > 0 ) {
vcListingURLs = gsub(
gsub(
unlist(
strsplit(
x = vcWebpageListings,
split = 'href'
)
),
pattern = '" target.*',
replacement = ''
),
pattern = '= *?"',
replacement = ''
)
print(paste('Adding',vcListingURLs))
vcURLsToScrapeFromThisPage = c(
vcURLsToScrapeFromThisPage,
vcListingURLs
)
} else {
# If there were no search results to be added from this page
# then get out of the loop
break
}
# Incrementing page number
iPageNo = iPageNo + 1
}
# Appending the results from this search to all the results from
# previous searches
vcURLsToScrape = c(
vcURLsToScrape,
vcURLsToScrapeFromThisPage
)
# Next search URL
i = i + 1
}
vcURLsToScrape
# rm(list = ls())
library(googlesheets)
library(data.table)
library(dplyr)
library(rjson)
cFileName = 'Automated House Leads 2018'
cResultsSheetName = 'CommonFloor'
cSearchURLPattern = 'commonfloor'
# Getting details from the Google sheet
# Code is same across websites
# =============================================================================
# Reading the entire sheet
AHH <- gs_title(cFileName)
# Reading the list of search URLs from this website
gsWebpageSearch <- AHH %>% gs_read(ws = "QueryURLs")
vcWebpageSearch = gsWebpageSearch$URL
vcWebpageSearch = vcWebpageSearch[
grepl(
tolower(vcWebpageSearch),
pattern = cSearchURLPattern
)
]
# Reading the listings already scraped from this website
gsListings <- AHH %>% gs_read(ws = cResultsSheetName)
vcAlreadySeenListings = gsListings$ZZURL
# Getting the filters to be applied to the new readings which get scraped
gsFilters <- AHH %>% gs_read(ws = paste0(cResultsSheetName, "Filters"))
# Getting the list of properties to scrape by scraping each search URL
# =============================================================================
# We'll populate this vector with the final URLs
vcURLsToScrape = c()
# This site lists a society as one entry which the user can
# click on to see the actual property entries. We'll need
# to scrape these society pages separately after we're done
# with the search URLs. This vector will store them.
vcWebpagesAddToSearch = c()
# counter to loop through all the search URLs
i = 1
if ( i > length(vcWebpageSearch) ) {
break
}
print(paste('Searching',vcWebpageSearch[i]))
# if the search page returns multiple pages of results then
# need to go through each of them
iPageNo = 1
vcURLsToScrapeFromThisPage = c()
print(paste('Page number',iPageNo))
# Substituting the page number in the search page URL
vcWebpage = readLines(
paste0(
gsub(
x = vcWebpageSearch[i],
pattern = 'page=.*?&',
replacement = paste0('page=',iPageNo, '&')
)
)
)
cStringToChopOffAfterSuggestion = 'No more matching properties for your search'
vcWebpage = vcWebpage[
grepl(
x = vcWebpage,
pattern = cStringToChopOffAfterSuggestion
)
]
vcWebpage
# Substituting the page number in the search page URL
vcWebpage = readLines(
paste0(
gsub(
x = vcWebpageSearch[i],
pattern = 'page=.*?&',
replacement = paste0('page=',iPageNo, '&')
)
)
)
vcWebpage
# Substituting the page number in the search page URL
vcWebpage = readLines(
paste0(
gsub(
x = vcWebpageSearch[i],
pattern = 'page=.*?&',
replacement = paste0('page=',iPageNo, '&')
)
)
)
vcURLsToScrapeFromThisPage = grep(
x = vcWebpage,
pattern = 'inforow clearfix',
value = T
)
vcURLsToScrapeFromThisPage
vcWebpage
